---
author: "Kevin Cheng"
title: "STA 380, Part 2: Exercises"
date: "August 17, 2020"
---

```{r wrap-hook, include=FALSE}
library(knitr)
hook_output = knit_hooks$get('output')
knit_hooks$set(output = function(x, options) {
  # this hook is used only when the linewidth option is not NULL
  if (!is.null(n <- options$linewidth)) {
    x = knitr:::split_lines(x)
    # any lines wider than n should be wrapped
    if (any(nchar(x) > n)) x = strwrap(x, width = n)
    x = paste(x, collapse = '\n')
  }
  hook_output(x, options)
})
```

1. Visual Story Telling Part 1: Green Buildings
===============================================    
```{r setup, include = FALSE}
library(dplyr)
library(ggplot2)
library(kknn)
library(quantmod)
library(mosaic)
library(foreach)
library(pander)
library(proxy)
library(RColorBrewer)
library(gridExtra)
library(ggpubr)
library(nycflights13)
library(maps)
library(ggthemes)
library(arules)
library(tm)
library(tm) 
library(magrittr)
library(randomForest)
library(caret)
library(e1071)

buildings = read.csv('https://raw.githubusercontent.com/jgscott/STA380/master/data/greenbuildings.csv')
green_buildings = subset(buildings, green_rating==1)
non_green_buildings = subset(buildings, green_rating!=1)
attach(buildings)

```

First, I looked into the distribution of leasing rate of green buildings and non-green buildings. Interestingly, the distribution of non-green buildings' leasing rate has a shoot up in the range below 10%. Therefore, I hold the same belief that these buildings are "weird" and should be removed from our analysis. 


```{r, echo=FALSE}
#plot subset (non_green_buildings))
hist(non_green_buildings$leasing_rate, 
     col= "darkslategray1",
     main = "Occupancy", 
     xlab = "Leasing Rate")

#plot subset (green_buildings)
hist(green_buildings$leasing_rate, col="lightgreen", type= "count", add=TRUE)

legend("topleft",
       legend=c('Non-Green buildings','Green buildings'),
       pch=c(15,15),
       col=c('darkslategray1','lightgreen'))
```

Second, the leasing rate for green buildings is highly left-skewed, so the median is a better estimation for building, which is 92.9%.  
``` {r, echo=FALSE}
d <- density(buildings$Rent)
plot(d, main="Rent to Density", xlab=("Rent"))
# polygon(d, col="lightgrey")

abline(v=median(buildings$Rent),col='mediumorchid1',lwd=3)
abline(v=mean(buildings$Rent),col='orange',lwd=3)

# abline(v=median(non_green_buildings$Rent),col='grey',lwd=2)
# abline(v=median(green_buildings$Rent),col='lightgreen',lwd=2)

legend("topright", cex=0.7,
       legend=c('Median Rent','Mean Rent'),
       lty=c(1,1), lwd=c(2,2),
       col=c('mediumorchid1','orange'))
```

Third, the way the author calculates the premium rent for green buildings is too generic as there are confounding variables. With these confounding variables, we are not sure how the green rating directly influences the rent. Age is one of the confounding variables. As shown in the plot, green buildings are highly concentrated in the lower range of age, which means they are relatively new, thus having higher rent.   
```{r confounding_age, echo=FALSE}
green_median = green_buildings[green_buildings$Rent == median(green_buildings$Rent),]
nongreen_median = non_green_buildings[non_green_buildings$Rent == median(non_green_buildings$Rent),]

plot(non_green_buildings$age, non_green_buildings$Rent, 
     col="grey", pch=21, cex=0.5,
     main="Age vs. Rent",
     xlab = "Buildings Age", 
     ylab="Annual Rent  ($/ sqft)")

abline(v=median(nongreen_median$age),col='grey',lwd=2)
abline(v=median(green_median$age),col='lightgreen',lwd=2)
points(green_buildings$age, green_buildings$Rent, col="lightgreen", pch=21, cex=0.5)

legend("topright", cex=0.7,
       legend=c('Non-Green buildings','Median Age Non-Green Buildings','Green buildings','Median Age Green Buildings'),
       lty=c(NA,1,NA,1),pch=c(21,NA,21,NA),
       col=c('grey','grey','lightgreen','lightgreen'))
```
We have seen that the analysis by stats guru is flawed since he fails to account for all the factors that affect the rent. Though green building might be a great idea in terms of rent revenue, the stats guru ignored too many factors into his analysis.

 
2. Visual Story Telling Part 2: Flights at ABIA  
===============================================  
```{r, include=FALSE}
#Import ABIA data
flight = read.csv("https://raw.githubusercontent.com/jgscott/STA380/master/data/ABIA.csv")
attach(flight)

#Turn catigorical variables into factors 
DayofMonth=as.factor(DayofMonth)
DayOfWeek=as.factor(DayOfWeek)
Dest=as.factor(Dest)
Diverted=as.factor(Diverted)
Cancelled=as.factor(Cancelled)
CancellationCode=as.factor(CancellationCode)
Month=as.factor(Month)
Origin=as.factor(Origin)
UniqueCarrier=as.factor(UniqueCarrier)

#Split delay time into delay(>0) and arrive in advance(<=0)
flight$Adelay=ifelse(flight$ArrDelay>0,flight$ArrDelay, NA)
flight$Aahead=ifelse(flight$ArrDelay<=0,-flight$ArrDelay,NA)
flight$Ddelay=ifelse(flight$DepDelay>0,flight$DepDelay,NA)
flight$Dahead=ifelse(flight$DepDelay<=0,-flight$DepDelay,NA)
```



```{r types of delay, echo=FALSE}
ggplot(flight, aes(x=Month, na.rm = TRUE, colour = Type)) + 
  geom_crossbar(aes(y = LateAircraftDelay, colour="Late Aircraft Delay"), stat = "summary", fun.y = "mean", na.rm = TRUE) + 
  geom_crossbar(aes(y = SecurityDelay, colour = "Security Delay"), stat = "summary", fun.y = "mean", na.rm = TRUE) +
  geom_crossbar(aes(y = NASDelay, colour = "NAS Delay"), stat = "summary", fun.y = "mean", na.rm = TRUE) +
  geom_crossbar(aes(y = WeatherDelay, colour = "Weather Delay"), stat = "summary", fun.y = "mean", na.rm = TRUE) +
  geom_crossbar(aes(y = CarrierDelay, colour = "Carrier Delay"), stat = "summary", fun.y = "mean", na.rm = TRUE) +
  ylab(label="Delay Time") + 
  xlab("Month")+
  theme_minimal() +
  scale_x_continuous(breaks=c(1,2,3,4,5,6,7,8,9,10,11,12)) +
  ggtitle('Five Types of Delay')
```
   
According to this plot, I can see late aircraft delay is much higher than any other delay types except for September and October. Overall, The most delays occur in the winter months.    
  
  
```{r Carrier Delay, echo=FALSE}
#Average arrival delay and carrier delay for each Carrier  

p=ggplot(flight, aes(x=UniqueCarrier, y=CarrierDelay, na.rm = TRUE))+
  stat_summary(fun.y="mean", geom="point",fill='steelblue1', na.rm = TRUE)+
  theme_minimal()+
  ggtitle('Average carrier delay')+
  xlab('Carrier Code')+
  ylab('Arrival Delay')+
  coord_cartesian(ylim=c(0,45))

ggarrange(p, ncol = 1, nrow = 1)
```
  
From these plots I can see that the higher average arrival delay for each carrier may not be due to their own fault. Airline YV and EV have both higher average arrival delay and average carrier delay, so it may be unwise to choose from these two carriers.   AircraftDelay has the largest delay time among all types of reasons. STL also ranks the first among this group. Therefore, based on the average total delay time for different airports, We can determine that STL is the worst airport to fly in. 

   
4. Market Segmentation  
======================   
-----------------------  
```{r, echo=FALSE}
social_market = read.csv('https://raw.githubusercontent.com/jgscott/STA380/master/data/social_marketing.csv')
attach(social_market)

```
  
I use TF-IDF to recalculate the weight of each term for every follower. TF stands for term-frequency, measuring how frequent a term occurs in a follower's tweets: the more frequent a term occurs, the more important it is to the follower; IDF stands for inverse-document-frequency, measuring how frequent the term occurs in the whole dataset: the more frequent a term occurs, the less important it is to every follower.   

```{r TFIDF, echo=FALSE}
#TFIDF
TF<-social_market[,-1]/rowSums(social_market[,-1])
tmp=sort(apply(TF,2,mean),decreasing=TRUE)
EXI_NUM<-apply(social_market[,-1]>0,2,function(x){table(x)['TRUE']})
IDF<-as.numeric(log(nrow(social_market)/EXI_NUM))
TFIDF = data.frame(t(t(TF)*IDF))
```

I use 'cosine' as a measurement for the similarity. It calculates the cosine of the angle between two vectors It measures difference in orientation instead of magnitude. For example, I have 3 follower A,B,C with features like A={'travelling':10,'cooking':5}, B={'travelling':20,'cooking':10}, C={'travelling':10,'cooking':12}, I would consider A more similar with B than C even though A and C are 'closer'.  
  
```{r hclust, echo=FALSE}
#hclust
d.cosine<-dist(TFIDF,method='cosine')
hc.ratio.cosine<-hclust(d.cosine,method='ward.D2')
```
By looking at different outputs of different Ks, I chose k=3 as our final parameter since its output makes more sense to us.  
  
```{r cluster 3, echo=FALSE}
#hclust
#choose cluster 3
out.cluster = cutree(hc.ratio.cosine,k=4)
TFIDF['cluster'] = out.cluster
tfidf<-c()
names<-c()
for(j in 1:3){
    cate = sort(apply(TFIDF[TFIDF['cluster']==j,-ncol(TFIDF)],2,mean),decreasing=TRUE)[1:3]
    name = names(cate)
    tfidf<-c(tfidf,unname(cate))
    names<-c(names,name)
}
cate.df = data.frame(names=names,tfidf_scores=tfidf,cluster=c(rep('A',length(tfidf)/3),rep('C',length(tfidf)/3),rep('B',length(tfidf)/3)))
cate.df$names<-factor(cate.df$names, levels=unique(cate.df$names))

colourCount = length(unique(cate.df$names))
getPalette = colorRampPalette(brewer.pal(8, "Set2"))

ggplot(cate.df,mapping=aes(x=cluster,y=tfidf_scores,fill=names))+geom_bar(stat='identity',position='dodge')+theme(panel.grid.minor=element_blank(),panel.grid.major=element_blank())+scale_fill_manual(values=getPalette(colourCount))+theme_minimal()+xlab("Cluster") + ylab("TF-IDF Score")+ ggtitle("Top 3 Names for Each Cluster")
```

From the topics of high TFIDF-socres in the clusters, I can infer that first cluster represents people who care a lot about health and fitness, mostly likely to be well-educated people and housewives; the second cluster represents college/high school students; the third cluster represents people who care about current events, most likely working people.  

 
5. Author Attribution   
======================   
```{r,echo = FALSE}
library(tm) 
library(magrittr)
library(randomForest)
library(caret)
library(e1071)

# Setitng the working directory
setwd("~/R/Predictive Modeling R Scripts/STA380_Part2_Scott_Exercises")

# Function to read the files
readerPlain = function(fname){
  readPlain(elem=list(content=readLines(fname)), 
            id=fname, language='en')
} 

# Get the filenames from the train data set
filenames <- list.files("./ReutersC50/C50train", recursive=TRUE)
myname = strsplit(filenames, "[/]")
author_name = NULL
dim(myname)
```
  
```{r,echo = FALSE}
for (i in 1:length(myname)) {
  author_name = c(author_name,myname[[i]][1])
}

class_labels_train = author_name
author_name_train = unique(author_name)
# Get the files in an array
file_list_train <- NULL
for (name in author_name_train){
  file_list_train <- c(file_list_train, Sys.glob(paste0('./ReutersC50/C50train/',name,'/*.txt')))
}
```
   
```{r,echo = FALSE}
# Read all files
all_docs_train = lapply(file_list_train, readerPlain)
mynames_train = file_list_train %>%
{ strsplit(., '/', fixed=TRUE) } %>%
{ lapply(., tail, n=2) } %>%
{ lapply(., paste0, collapse = '') } %>%
  unlist
# create a dataframe with doc_id as author-article and text as the text in that article
text_vector_train <- NULL
for(i in 1:length(mynames_train)){
  text_vector_train <- c(text_vector_train, paste0(content(all_docs_train[[i]]), collapse = " "))
}
# dataframe with text and document_id
text_df_train <- data.frame(doc_id = mynames_train,
                            text = text_vector_train)
## once you have documents in a vector, you 
## create a text mining 'corpus' with: 
train_raw <- VCorpus(DataframeSource(text_df_train))
## Some pre-processing/tokenization steps.
my_documents_train = train_raw
my_documents_train = tm_map(my_documents_train, content_transformer(tolower))
my_documents_train = tm_map(my_documents_train, content_transformer(removeNumbers))
my_documents_train = tm_map(my_documents_train, content_transformer(removePunctuation))
my_documents_train = tm_map(my_documents_train, content_transformer(stripWhitespace))
# Removing stop words
my_documents_train = tm_map(my_documents_train, content_transformer(removeWords), stopwords("en"))
## create a doc-term-matrix
DTM_train = DocumentTermMatrix(my_documents_train)
# Remove sparse terms
DTM_train = removeSparseTerms(DTM_train, 0.99)
DTM_train # now 3325 terms (versus ~32570 terms before)
# Now, let us repeat the above process of creating DTM for the test data
filenames <- list.files("./ReutersC50/C50test", recursive=TRUE)
myname = strsplit(filenames, "[/]")
author_name = NULL
for (i in 1:length(myname)) {
  author_name = c(author_name,myname[[i]][1])
}
class_labels_test = author_name
author_name = unique(author_name)
file_list_test = NULL
#class_labels_test = NULL
for (each in author_name) {
  file_list_test = c(file_list_test,Sys.glob(paste0('./ReutersC50/C50test/',each,'/*.txt')))
}
all_docs_test = lapply(file_list_test, readerPlain)
mynames_test = file_list_test %>%
{ strsplit(., '/', fixed=TRUE) } %>%
{ lapply(., tail, n=2) } %>%
{ lapply(., paste0, collapse = '') } %>%
  unlist
# create a dataframe with doc_id as author-article and text as the text in that article
text_vector_test <- NULL
for(i in 1:length(mynames_test)){
  text_vector_test <- c(text_vector_test, paste0(content(all_docs_test[[i]]), collapse = " "))
}
# dataframe with text and document_id
text_df_test <- data.frame(doc_id = mynames_test,
                            text = text_vector_test)
# convert the dataframe to a Corpus
test_raw <- VCorpus(DataframeSource(text_df_test))
## Some pre-processing/tokenization steps.
## tm_map just maps some function to every document in the corpus
my_documents_test = test_raw
my_documents_test = tm_map(my_documents_test, content_transformer(tolower))
my_documents_test = tm_map(my_documents_test, content_transformer(removeNumbers))
my_documents_test = tm_map(my_documents_test, content_transformer(removePunctuation))
my_documents_test = tm_map(my_documents_test, content_transformer(stripWhitespace))
# Removing stop words
my_documents_test = tm_map(my_documents_test, content_transformer(removeWords), stopwords("en"))
## create a doc-term-matrix
DTM_test = DocumentTermMatrix(my_documents_test)
summary(Terms(DTM_test) %in% Terms(DTM_train))
```
There are 32,589 words in document-term matrix for the test data, however there are only 3325 words which are also common in the train data set. So, let us drop the remaining words for the classification problem. This is however not an optimal solution, as I am dropping many words.  

```{r,echo = FALSE}
# A suboptimal but practical solution: ignore words you haven't seen before
# can do this by pre-specifying a dictionary in the construction of a DTM
DTM_test2 = DocumentTermMatrix(my_documents_test,
                               control = list(dictionary=Terms(DTM_train)))
# Now checking whether I have all the words from train in test
summary(Terms(DTM_test2) %in% Terms(DTM_train))
```
  
**Create the TF-IDF matrix for test and train data:**  
```{r,echo = FALSE}
# Now lets create TF-IDF matrix for train and test
tfidf_train = weightTfIdf(DTM_train)
tfidf_test = weightTfIdf(DTM_test2)
# Converting tfidf_train to matrix
tfidf_train = as.matrix(tfidf_train)
tfidf_train = tfidf_train[ , apply(tfidf_train, 2, var) != 0]
```
3325 words are still high to conduct classification. Thus I will reduce the dimensions using Principal Component Analysis. I will run the PCAs on the train data set and take the top words which explain 75% of the variability in data.  
  
  
**Run PCA on TF-IDF to reduce the number of words:**  
```{r PCA,echo = FALSE}
# Run PCA on TF-IDF to reduce the number of words
pc_train = prcomp(as.matrix(tfidf_train), scale=TRUE)
# Let's use the top PCs which explain 75% of the variability. So I will take first 330 PCs
X_train = pc_train$x[,1:330]
X_train  = cbind(X_train,class_labels_train)
# Now scale the test data
tfidf_test = as.matrix(tfidf_test)
#tfidf_test = tfidf_test[ , apply(tfidf_test, 2, var) != 0]
#Scaling the test data and applying PCs from train
scaled_tfidf_test = scale(tfidf_test, center=TRUE, scale=TRUE)
X_test <- scaled_tfidf_test %*% pc_train$rotation[,1:330]
#X_test_pc <- as.data.frame(X_test_pc)
#X_test = predict(pc_train, scaled_tfidf_test)
X_test = X_test[,1:330]
```
Based on the PCA on train data, I can say that 330 words define 75% of the variability. Using the PCAs on the train data set, I predicted the PCAs on test data set. I will use these data sets for our classification of the authors.    
  
Classification - Random Forest.  
---------------------------------------   
After running Random Forest I have an acuracy of 59%.  
```{r Random Forest,echo = FALSE}
X_train = as.data.frame(X_train)
for (name in names(X_train)){
  if (name == "class_labels_train"){
    next
  }else{
    X_train[[name]] <- as.numeric(as.character(X_train[[name]]))
  }
}
X_test = as.data.frame(X_test)
set.seed(99)
author.rf = randomForest(class_labels_train ~., 
                         data = X_train,
                         ntrees = 500,
                         importance = TRUE)
predict_test = predict(author.rf,X_test,type="response")
rf_confusion_matrix = table(predict_test,class_labels_test)
accuracy = sum(diag(rf_confusion_matrix)) / sum(rf_confusion_matrix)
accuracy
```
     
    
However, let's look at how the accuracy varies for different authors:    
```{r Accuracy variance,echo = FALSE}
library(dplyr)
accurate_authors = as.data.frame(cbind(unique(class_labels_train),
                                       diag(rf_confusion_matrix)))
colnames(accurate_authors) = c("Authors","Correct_Predictions")
accurate_authors$Correct_Predictions =  as.numeric(as.character(accurate_authors$Correct_Predictions))
accurate_authors$Authors = as.character(accurate_authors$Authors)
accurate_authors_sorted = accurate_authors[order(-accurate_authors$Correct_Predictions),]
accurate_authors_sorted$percentage_accuracy = accurate_authors_sorted$Correct_Predictions/50
#head(accurate_authors_sorted,10)
rownames(accurate_authors_sorted) <- NULL
pander(head(accurate_authors_sorted,10))
```
  
```{r, echo=FALSE}
# Plotting the top accurately predicted authors
top_n(accurate_authors_sorted, n=5, percentage_accuracy) %>%
          ggplot(., aes(x=reorder(Authors,-percentage_accuracy), y=percentage_accuracy))+
  geom_bar(stat = "identity",width = 0.4) +
  scale_y_continuous(labels = scales::percent,expand = c(0,0),limits = c(0,1)) +
  labs(title="% Documents Correctly Predicted for Top Predicted Authors", fontface='bold')+
  theme_minimal()+
  theme(axis.text.x = element_text(colour="grey20",size=9,angle=0,hjust=0.5,vjust=0.2),
        axis.text.y = element_text(colour="grey20",size=11,angle=0),
        axis.title.x = element_text(colour="grey20",size=11,hjust=0.5),
        axis.title.y = element_text(colour="grey20",size=11,hjust=0.5))+
  ylab("% Accuracy") +
  xlab ("Authors") + 
  geom_text(aes(label=paste0(sprintf("%.0f", percentage_accuracy*100),"%")), vjust = 1, size=4,fontface='bold',colour = 'white')

#PLot the authors which are not predicted that well - 
top_n(accurate_authors_sorted, n=-5, percentage_accuracy) %>%
          ggplot(., aes(x=reorder(Authors,-percentage_accuracy), y=percentage_accuracy))+
  geom_bar(stat = "identity",width = 0.4) +
  scale_y_continuous(labels = scales::percent,expand = c(0,0),limits = c(0,1)) +
  labs(title="% Documents Correctly Predicted for Bottom Predicted Authors", fontface='bold')+
  theme_minimal()+
  theme(axis.text.x = element_text(colour="grey20",size=9,angle=0,hjust=0.5,vjust=0.2),
        axis.text.y = element_text(colour="grey20",size=11,angle=0),
        axis.title.x = element_text(colour="grey20",size=11,hjust=0.5),
        axis.title.y = element_text(colour="grey20",size=11,hjust=0.5))+
  ylab("% Accuracy") +
  xlab ("Authors") + 
  geom_text(aes(label=paste0(sprintf("%.0f", percentage_accuracy*100),"%")), vjust = 1, size=4,fontface='bold',colour = 'white')
```

Overall, the outputs of the model does give a accuracy of ~58%. Overall accuracy seems to be low as there are some authors who are not predicted that well. One potential reason behind this could be that I have dropped quite a few words form the train and test data sets. However, there are many more words in the test data (which if incorporated could improve accuracies).    


6. Association Rule Mining  
============================  
```{r groceries init, warning=FALSE, message=FALSE, include=FALSE}
library(arules)  
library(reshape)
library(arulesViz)
library(tidyverse)

# Read in groceries from file
groceries_raw = scan("https://raw.githubusercontent.com/jgscott/STA380/master/data/groceries.txt", what = "", sep = "\n")
head(groceries_raw)
str(groceries_raw)
#summary(groceries_raw)
```

I transform the data into a "transactions" class before applying the apriori algorithm.
The summary of the dataset reveals the following:
1. There are total of 9835 transactions in our dataset
2. Whole milk is the present in 2513 baskets and is the most frequently bought item

```{r echo=FALSE, include=FALSE}
## Process the data and cast it as a "transactions" class
groceries = strsplit(groceries_raw, ",")
groctrans = as(groceries, "transactions")
#summary(groctrans)
```

```{r echo=FALSE}
itemFrequencyPlot(groctrans, topN = 10)
```

**Let's explore rules with support > 0.05, confidence > 0.2 and length <= 2 using the 'apriori' algorithm**
We also notice that most relationships in this item set include whole milk, yogurt and rolls/buns which is in accordance with the transaction frequency plot we saw earlier. These are some of the most frequently bought items.

```{r echo=FALSE, include=FALSE}
grocrules_1 = apriori(groctrans, 
                     parameter=list(support=0.05, confidence=.2, minlen=2))
```

```{r echo=FALSE}
arules::inspect(grocrules_1)
plot(grocrules_1, method='graph')
```

**Let's decrease support further and increase confidence slightly with support > 0.03, confidence > 0.3 and length <= 2**

This item set contains 14 rules and includes a lot more items. However, whole milk still seems to be a common item.

```{r echo=FALSE, include=FALSE}
grocrules_2 = apriori(groctrans, 
                     parameter=list(support=0.03, confidence=.3, minlen=2))
arules::inspect(grocrules_2)
```

```{r echo=FALSE}
plot(head(grocrules_2,15,by='lift'), method='graph')
```

In conclusion, whole milk is the most common item purchased by customers. Whole milk, yogurt, rolls/buns are all often purchased together, which could be due to the close proximity to one another in a grocery store. 